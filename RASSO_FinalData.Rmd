---
title: "RASSO_finaldata"
author: "Kejing Yan"
date: "2023-09-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(knitr)
library(formatR)
library(glmnet)
library(ridge)
library(aod)
library(MASS)
library(graphics)
theme_update(plot.title = element_text(hjust = 0.5))
opts_chunk$set(tidy.opts=list(width.cutoff=50),tidy=TRUE)
```

```{r}
# Read data
obesity_data <- read.csv('Obesity survey_final.csv')
# Clean data
obesity_data <- subset(obesity_data, select = -c(X.Diagnosis., X.Medication., X, X.Experience., X.1, X.WT.loss.Program., X.Age., X.Income., X.Location.))
obesity_data <- na.omit(obesity_data)

head(obesity_data)
```


```{r}
# Change all yes to 1, and all no to 0
obesity_data_partial <- ifelse(obesity_data[,8:length(obesity_data)] == 1, 1, 0)
obesity_data_new <- data.frame(
  obesity_data[, 1:7],
  obesity_data_partial
)
head(obesity_data_new)
head(obesity_data_partial)
# Lasso method
y <- obesity_data_new$Surgery
x <- data.matrix(obesity_data_new[,-c(1,2)])
```

```{r}
#LASSO
#last 16 factors are forced to stay in the model
cv_model <- cv.glmnet(x, y, alpha = 1, nfolds = 5,
                      penalty.factor = c(rep(1, ncol(x) - 16), rep(0, 16)))

best_lambda <- cv_model$lambda.min
best_lambda
plot(cv_model)

best_model <- glmnet(x, y, alpha = 1, lambda = best_lambda, nfolds = 5,
                     penalty.factor = c(rep(1, ncol(x) - 16), rep(0, 16)))
coef(best_model)
```

```{r}
# Set the number of bootstrap samples
B <- 1000

# Create an empty matrix to store the bootstrap results
boot_coefs <- matrix(0, ncol = ncol(obesity_data_partial)-1, nrow = B)

# Lasso method with bootstrap for every predictor
# for(i in 1:(ncol(obesity_data_partial)-1)){
# Extract the predictor and response variables
y <- obesity_data_new$Surgery
x <- data.matrix(obesity_data_partial[,c(3:ncol(obesity_data_partial))])

# Run bootstrap
for(j in 1:B){
  index <- sample(nrow(x), replace = TRUE)
  x_boot <- x[index,]
  y_boot <- y[index]
  
  # Run Lasso with cross-validation
  cv_model <- cv.glmnet(x_boot, y_boot, alpha = 1)
  best_lambda <- cv_model$lambda.min
  
  # Run Lasso with the best lambda
  best_model <- glmnet(x_boot, y_boot, alpha = 1, lambda = best_lambda)
  
  # Store the coefficient for predictor i and bootstrap sample j
  boot_coefs[j,] <- coef(best_model)[,1]
}
```

```{r}
# Plot the distributions of the coefficients for each predictor
# par(mfrow = c(6, 5), mar = c(2,2,2,2))
for(i in 1:(ncol(obesity_data_partial)-1)){
  hist(boot_coefs[,i], main = colnames(obesity_data_partial)[i+1], col = 'light blue')
  ci <- quantile(boot_coefs[,i], c(0.025, 0.975))
  print(ci)
  abline(v = ci, col = "red", lwd = 2)
}
```

***Multivariate Logistic Regression***
```{r}
# Logistic regression for selected variables
obesity_data_selected <- obesity_data_new[-c(1,4,8,9,21,29)]
#clean data (remove NAs)
x.full <- data.matrix(obesity_data_selected[,-1])
y.full <- obesity_data_selected$Surgery
cv.logi <- cv.glmnet(x.full, y.full, alpha = 1, nfolds = 5, family = 'binomial',
                      penalty.factor = c(rep(1, ncol(x.full) - 16), rep(0, 16)))
```

```{r}
best_lambda <- cv.logi$lambda.min
best_lambda
plot(cv.logi)

best_model <- glmnet(x.full, y.full, alpha = 1, lambda = best_lambda, 
                     family = 'binomial', 
                     penalty.factor = c(rep(1, ncol(x.full) - 16), rep(0, 16)))
coef(best_model)
```

```{r}
# Wald test
# Using normal multivariate logistic regression
logi_model <- glm(Surgery ~., data = obesity_data_selected, family = 'binomial')
vcov <- vcov(logi_model)
summary(logi_model)
wald.test(coef(logi_model), Sigma = vcov, Terms = 1:29)
#reject H0 as pval small
```

```{r}
# Odds ratio of each variable
odds_ratio <- exp(coef(logi_model))
# Intepretation example: if the odds ratio for x1 is 1.5, it means that for a unit increase in x1, the odds of the positive outcome increase by a factor of 1.5, or 50%. Conversely, if the odds ratio for x1 is 0.5, it means that for a unit increase in x1, the odds of the positive outcome decrease by a factor of 0.5, or 50%.

upperbound <- exp(coef(logi_model) + 1.96*summary(logi_model)$coefficients[, 2])
lowerbound <- exp(coef(logi_model) - 1.96*summary(logi_model)$coefficients[, 2])

#95% CI for odds ratio
cbind(lowerbound, odds_ratio, upperbound)
```

***95% CI of coefficients from general logistic regression
```{r}
confint(logi_model, level = 0.95)
```



```{r}
# 95% Confidence Interval
# Set the number of bootstrap samples
B <- 1000

# Create an empty matrix to store the bootstrap results
boot_coefs <- matrix(0, ncol = ncol(obesity_data_selected), nrow = B)

# Lasso method with bootstrap for every predictor
# for(i in 1:(ncol(obesity_data_partial)-1)){
# Extract the predictor and response variables
y <- obesity_data_selected$Surgery
x <- data.matrix(obesity_data_selected[,-1])

# Run bootstrap
#for(j in 1:B){
#  index <- sample(nrow(x), replace = TRUE)
#  x_boot <- x[index,]
#  y_boot <- y[index]
# 
#  # Run Lasso with cross-validation
#  cv_model <- cv.glmnet(x_boot, y_boot, alpha = 1)
#  best_lambda <- cv_model$lambda.min
#  
  # Run Lasso with the best lambda
#  best_model <- glmnet(x_boot, y_boot, alpha = 1, lambda = best_lambda)
  
  # Store the coefficient for predictor i and bootstrap sample j
#  boot_coefs[j,] <- coef(best_model)[,1]
#}
```

```{r}
# Plot the distributions of the coefficients for each predictor
# par(mfrow = c(6, 5), mar = c(2,2,2,2))
#for(i in 1:(ncol(obesity_data_partial))){
#  hist(boot_coefs[,i], main = colnames(obesity_data_selected)[i+1], col = 'light blue')
#  ci <- quantile(boot_coefs[,i], c(0.025, 0.975))
#  print(ci)
#  abline(v = ci, col = "red", lwd = 2)
#}
```

***95% CI by extracting variance-covariance matrix
```{r}
# 95% Confidence Interval
ridge_se <- function(xs,y,yhat,my_mod){
  # Note, you can't estimate an intercept here
  n <- dim(xs)[1]
  k <- dim(xs)[2]
  sigma_sq <- sum((y-yhat)^2)/ (n-k)
  lam <- my_mod$lambda.min
  if(is.null(my_mod$lambda.min)==TRUE){lam <- 0}
  i_lams <- Matrix(diag(x=1,nrow=k,ncol=k),sparse=TRUE)
  xpx <- t(xs)%*%xs
  xpxinvplam <- solve(xpx+lam*i_lams)
  var_cov <- sigma_sq * (xpxinvplam %*% xpx %*% xpxinvplam)
  se_bs <- sqrt(diag(var_cov))
  print('NOTE: These standard errors are very biased.')
  return(se_bs)
}
ridge <- linearRidge(y.full~x.full-1,scaling='scale',lambda=cv.logi$lambda.min)
lmod2 <-summary(lm(y.full~scale(x.full)-1))

r_yhat   <- predict(cv.logi ,newx = x.full, s='lambda.min')
l_yhat <- predict(best_model,newx = x.full, s='lambda.min')
ro_yhat  <- predict(cv.logi, newx = x.full)

rmod_ses        <- ridge_se(x.full, y.full, r_yhat, cv.logi)
lmod_ses_GLMNET <- ridge_se(x.full, y.full, l_yhat, best_model)
romod_ses       <- data.frame(summary(ridge)$summaries$summary1[1])
names(romod_ses)<- c('Estimate','Scale_Estimate','Std_Error','T_value','P_Value')
lmod_ses        <- data.frame(lmod2[4])
names(lmod_ses) <- c('Estimate','Std_Error','T_value','P_Value')
lmod_ses <- data.frame(Names=row.names(lmod_ses),
                       OLS=lmod_ses$Std_Error)

std_errs <- lmod_ses
std_errs$Ridge <- rmod_ses
std_errs$OLS_GLMNET <- lmod_ses_GLMNET
std_errs$OLS_Diffs <- with(std_errs,(OLS-OLS_GLMNET))
std_errs$Orig_Ridge <- romod_ses$Std_Error
std_errs$Ridge_Diff <- with(std_errs,Ridge-Orig_Ridge)

CI <- cbind(best_model$beta - std_errs$OLS_GLMNET, best_model$beta, best_model$beta + std_errs$OLS_GLMNET)
colnames(CI) <- c("LowerBoundary", "Coefficient", "UpperBoundary")
CI
```
